\documentclass{sig-alternate}
\usepackage{subfigure,algorithm,algorithmicx,algpseudocode}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{document}

\title{Functions of Linear Regression}

\numberofauthors{1}
\author{
	% 1st. author
	\alignauthor
	Xueyuan Gong\\
	\affaddr{Department of Computer and Information Science}\\
	\affaddr{Faculty of Science and Technology}\\
    \affaddr{University of Macau}\\
    \affaddr{Macau, China}\\
    \email{\{yb47453\}@umac.mo}
}

\maketitle

Given a data set $X\in \mathbb{R}^{n\times (m-1)}$ and a label set $Y\in \mathbb{R}^{n\times 1}$. Let two indices $i$ and $j$ be subject to $1\leq i\leq n$ and $2\leq j \leq m$ respectively. Thus, a data value is denoted as $X^{i}_{j}$ and a label value is denoted as $Y^{i}$. For all $X^{i}$, insert $X^{i}_{1}=1$ so that $X\in \mathbb{R}^{n\times m}$ and $1\leq j \leq m$. Initialize weight matrix $W\in \mathbb{R}^{1\times m}$ and thus a weight value is denoted as $W_{j}$.

Hypothesis function is defined in Equation \eqref{eq:hf}.

\begin{equation}
\label{eq:hf}
	H(X^{i})=\sum_{j=1}^{m}W_{j}X^{i}_{j}
\end{equation}

Loss function is defined in Equation \eqref{eq:lf}.

\begin{equation}
\label{eq:lf}
	C(X^{i})=H(X^{i})-Y^{i}
\end{equation}

Cost function is defined in Equation \eqref{eq:cf}.

\begin{equation}
\label{eq:cf}
	\begin{aligned}
	J(W)&=\frac{1}{2n}\sum_{i=1}^{n}C^2(X^{i})\\
		&=\frac{1}{2n}\sum_{i=1}^{n}(\sum_{j=1}^{m}W_{j}X^{i}_{j}-Y^{i})^{2}
	\end{aligned}
\end{equation}

The partial derivative of $J(W)$ with respect to $W_{j}$ is given as follows:

\begin{align*}
	\frac{\partial{J(W)}}{\partial{W_{j}}}
	&=\frac{1}{2n}\sum_{i=1}^{n}\frac{\partial{C^2(X^{i})}}{\partial{W_{j}}}\\
	&=\frac{1}{2n}\sum_{i=1}^{n}\frac{\partial{C^2(X^{i})}}{\partial{C(X^{i})}}\cdot \frac{\partial{C(X^{i})}}{\partial{H(X^{i})}}\cdot \frac{\partial{H(X^{i})}}{\partial{W_{j}}}\\
	&=\frac{1}{2n}\sum_{i=1}^{n}2C(X^{i})\cdot 1\cdot X^{i}_{j}\\
	&=\frac{1}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}
\end{align*}

Thus, for all $W_{j}$, the update function is given as follows:

\begin{align*}
	W_{j}=W_{j}-\frac{\gamma}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}
\end{align*}
where $\gamma$ is the learning rate given by users.

\end{document}
