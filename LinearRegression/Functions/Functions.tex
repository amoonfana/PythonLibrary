\documentclass[conference]{IEEEtran}
\usepackage{amsfonts,amsmath,graphicx,subfigure,algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Functions of Linear Regression}

\author{
	\IEEEauthorblockN{Xueyuan Gong}
	\IEEEauthorblockA{Department of Computer and Information Science\\University of Macau\\Macau, China\\\{yb474530\}@umac.mo}
}

\maketitle

\section{Linear Regression}

Given a data set $X\in \mathbb{R}^{n\times m}$ and a label set $Y\in \mathbb{R}^{n\times 1}$. Let two indices $i$ and $j$ be subject to $1\leq i\leq n$ and $1\leq j \leq m$ respectively. Thus, a data value is denoted as $X^{i}_{j}$ and a label value is denoted as $Y^{i}$. Initialize weight matrix $W\in \mathbb{R}^{1\times m}$ and bias $b$, thus a weight value is denoted as $W_{j}$.

Hypothesis function is defined in Equation \eqref{eq:hf}.

\begin{equation}
\label{eq:hf}
	H(X^{i})=\sum_{j=1}^{m}W_{j}X^{i}_{j}+b
\end{equation}

Loss function is defined in Equation \eqref{eq:lf}.

\begin{equation}
\label{eq:lf}
	C(X^{i})=H(X^{i})-Y^{i}
\end{equation}

Cost function is defined in Equation \eqref{eq:cf}.

\begin{equation}
\label{eq:cf}
	\begin{aligned}
	J(W)&=\frac{1}{2n}\sum_{i=1}^{n}C^2(X^{i})\\
		&=\frac{1}{2n}\sum_{i=1}^{n}(\sum_{j=1}^{m}W_{j}X^{i}_{j}+b-Y^{i})^{2}
	\end{aligned}
\end{equation}

The partial derivative of $J(W)$ with respect to $W_{j}$ is given as follows:

\begin{align*}
	\frac{\partial{J(W)}}{\partial{W_{j}}}
	&=\frac{1}{2n}\sum_{i=1}^{n}\frac{\partial{C^2(X^{i})}}{\partial{W_{j}}}\\
	&=\frac{1}{2n}\sum_{i=1}^{n}\frac{\partial{C^2(X^{i})}}{\partial{C(X^{i})}}\cdot \frac{\partial{C(X^{i})}}{\partial{H(X^{i})}}\cdot \frac{\partial{H(X^{i})}}{\partial{W_{j}}}\\
	&=\frac{1}{2n}\sum_{i=1}^{n}2C(X^{i})\cdot 1\cdot X^{i}_{j}\\
	&=\frac{1}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}
\end{align*}

Thus, for all $W_{j}$, the update function is given as follows:

\begin{align*}
	W_{j}=W_{j}-\frac{\gamma}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}
\end{align*}
where $\gamma$ is the learning rate given by users.

\end{document}
