\documentclass[conference]{IEEEtran}
\usepackage{amsfonts,amsmath,amssymb,graphicx,subfigure,algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Functions of Neural Network}

\author{
	\IEEEauthorblockN{Xueyuan Gong}
	\IEEEauthorblockA{Department of Computer and Information Science\\University of Macau\\Macau, China\\\{yb47453\}@umac.mo}
}

\maketitle

\section{Neural Network}

Given a data set $X\in \mathbb{R}^{n\times m_{0}}$ and a label set $Y\in \mathbb{R}^{n\times 1}$. Thus, a data value is denoted as $X^{i}_{j}$ and a label value is denoted as $Y^{i}$. Let two indices $i$ and $j$ be subject to $1\leq i\leq n$ and $1\leq j \leq m$ respectively.

Suppose the network has $l$ layers, where the size of each layer $L_{k}$ is denoted as $m_{k}$. Note the input/first layer $L_{1}$ accepts $X^{i}\in \mathbb{R}^{1\times m_{0}}$ as input data. Initialize weight matrix list $W$ and bias list $b$ containing $l-1$ matrices and biases respectively, where each matrix $W^{z}\in \mathbb{R}^{m_{k}\times m_{k+1}}$ and each bias $b^{z}\in \mathbb{R}^{1\times m_{k+1}}$

 and biases $b$, thus a weight value is denoted as $W_{j}$.

We define the linear function in Equation \eqref{eq:lnf}.

\begin{equation}
\label{eq:lnf}
	Z^{i}=\sum_{j=1}^{m}W_{j}X^{i}_{j}+b
\end{equation}

We define activation function as sigmoid function and thus the hypothesis function is given in Equation \eqref{eq:hf}.

\begin{equation}
\label{eq:hf}
	H(X^{i}) = G(Z^{i})=\frac{1}{1+e^{-Z^{i}}}
\end{equation}

Loss function is defined in Equation \eqref{eq:lsf}.

\begin{equation}
\label{eq:lsf}
	C(X^{i})=-Y^{i}\log{(H(X^{i}))}-(1-Y^{i})\log{(1-H(X^{i}))}
\end{equation}

Cost function is defined in Equation \eqref{eq:cf}.

\begin{equation}
\label{eq:cf}
	\begin{aligned}
	J(W)&=\frac{1}{n}\sum_{i=1}^{n}C(X^{i})\\
		&=-\frac{1}{n}\sum_{i=1}^{n}[Y^{i}\log{(H(X^{i}))}+(1-Y^{i})\log{(1-H(X^{i}))}]
	\end{aligned}
\end{equation}

The partial derivative of $J(W)$ with respect to $W_{j}$ is given as follows:

\begin{align*}
	&\frac{\partial{J(W)}}{\partial{W_{j}}}\\
	&=\frac{1}{n}\sum_{i=1}^{n}\frac{\partial{C(X^{i})}}{\partial{W_{j}}}\\
	&=\frac{1}{n}\sum_{i=1}^{n}[\frac{\partial{Y^{i}\log{(H(X^{i}))}}}{\partial{W_{j}}}+\frac{\partial{(1-Y^{i})\log{(1-H(X^{i}))}}}{\partial{W_{j}}}]
\end{align*}

For simplicity, we calculate the equation separately:

\begin{align*}
	&\frac{\partial{Y^{i}\log{(H(X^{i}))}}}{\partial{W_{j}}}\\
	&=Y^{i}\frac{\partial{\log{(H(X^{i}))}}}{\partial{H(X^{i})}}\cdot \frac{\partial{H(X^{i})}}{\partial{(Z^{i})}}\cdot \frac{\partial{(Z^{i})}}{\partial{W_{j}}}\\
	&=Y^{i}\frac{1}{H(X^{i})}\cdot H(X^{i})(1-H(X^{i}))\cdot X^{i}_{j}\\
	&=Y^{i}(1-H(X^{i}))X^{i}_{j}\\
	\\
	&\frac{\partial{(1-Y^{i})\log{(1-H(X^{i}))}}}{\partial{W_{j}}}\\
	&=(1-Y^{i})\frac{\partial{\log{(1-H(X^{i}))}}}{\partial{(1-H(X^{i}))}}\cdot \frac{\partial{(1-H(X^{i}))}}{\partial{H(X^{i})}} \cdot \frac{\partial{H(X^{i})}}{\partial{(Z^{i})}}\cdot \frac{\partial{(Z^{i})}}{\partial{W_{j}}}\\
	&=(1-Y^{i})\frac{1}{1-H(X^{i})}\cdot (-1)\cdot H(X^{i})(1-H(X^{i}))\cdot (X^{i}_{j})\\
	&=(Y^{i}-1)H(X^{i})X^{i}_{j}
\end{align*}

\begin{align*}
	\therefore~
	\frac{\partial{J(W)}}{\partial{W_{j}}}
	&=-\frac{1}{n}\sum_{i=1}^{n}[Y^{i}(1-H(X^{i}))X^{i}_{j}+(Y^{i}-1)H(X^{i})X^{i}_{j}]\\
	&=\frac{1}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}
\end{align*}

Thus, for all $W_{j}$, the update function is given as follows:

\begin{align*}
	W_{j}=W_{j}-\frac{\gamma}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}
\end{align*}
where $\gamma$ is the learning rate given by users.

\section{Regularized Logistic Regression}
The cost function of regularized logistic regression is given in Equation \eqref{eq:rcf}.

\begin{equation}
\label{eq:rcf}
	J(W)=\frac{1}{n}\sum_{i=1}^{n}C(X^{i})+\frac{\lambda}{2n}\sum_{j=1}^{m}(W_{j})^{2}
\end{equation}
where $\lambda$ is the regularization ratio set by users.

The partial derivative of $J(W)$ with respect to $W_{j}$ is given as follows:

\begin{align*}
	\frac{\partial{J(W)}}{\partial{W_{j}}}
	&=\frac{1}{n}\sum_{i=1}^{n}\frac{\partial{C(X^{i})}}{\partial{W_{j}}}+\frac{\lambda}{2n}\frac{\partial{\sum_{j=1}^{m}(W_{j})^{2}}}{\partial{W_{j}}}\\
	&=\frac{1}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}+\frac{\lambda}{n}W_{j}
\end{align*}

However, we do not want to regularize the weight of the constant factor $W_{1}$. Thus, for all $W_{j}$ and $b$, the update function is given as follows:

\begin{align*}
	W_{j}&=W_{j}-\frac{\gamma}{n}[\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}+\lambda W_{j}]\\
	b&=b-\frac{\gamma}{n}\sum_{i=1}^{n}(H(X^{i})-Y^{i})X^{i}_{j}
\end{align*}

\end{document}
